{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return word_tokenize(sent.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_newlines(text):\n",
    "    if text[0] == '\\n':\n",
    "        text = text[-len(text)+1:]\n",
    "    elif text[-1] == '\\n':\n",
    "        text = text[:len(text)-1]\n",
    "    return text.lower()\n",
    "\n",
    "def get_cdata(context_file):\n",
    "    c = context_file.read()\n",
    "    lines = re.split('\\n' + '-'*30 + '\\n', c)[:-1]\n",
    "    return lines\n",
    "\n",
    "def get_qdata(xfile):\n",
    "    final = []\n",
    "    line = []\n",
    "    for l in xfile:\n",
    "        l = remove_newlines(l)\n",
    "        if l != '-'*30:\n",
    "            line.append(l)\n",
    "        else:\n",
    "            final.append(line)\n",
    "            line = []\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_file = open('./data/train_context', 'r')\n",
    "questions_file = open('./data/train_question', 'r')\n",
    "answers_file = open('./data/train_answer', 'r')\n",
    "context_data = get_cdata(context_file)\n",
    "questions_data = get_qdata(questions_file)\n",
    "answers_data = get_qdata(answers_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for cid, context in enumerate(context_data):\n",
    "    for qid, question in enumerate(questions_data[cid]):\n",
    "        data.append((context.strip(), question.strip(), answers_data[cid][qid].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for story, q, answer in data:\n",
    "    vocab |= set(tokenize(story) + tokenize(q) + tokenize(answer))\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in data)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_stories(data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in tokenize(story)])\n",
    "        queries.append([word_idx[w] for w in tokenize(query)])\n",
    "        answers.append(word_idx[w] for w in tokenize(answer))\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_train, queries_train, answers_train = vectorize_stories(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'idiot', \"'s\", 'noob']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.word_tokenize('hello idiot\\'s noob')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
