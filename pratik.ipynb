{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/pratik/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, RepeatVector, Masking, Dropout, Flatten, Activation, Reshape, Lambda, Permute,Add ,merge, multiply, concatenate\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.activations import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_file = open(os.path.join('./data/', 'train_context'), 'r')\n",
    "c = context_file.read()\n",
    "context = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "question_file = open(os.path.join('./data/', 'train_question'), 'r')\n",
    "c = question_file.read()\n",
    "questions = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "answer_file = open(os.path.join('./data/', 'train_answer'), 'r')\n",
    "c = answer_file.read()\n",
    "answers = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "span_file = open(os.path.join('./data/', 'train_span'), 'r')\n",
    "c = span_file.read()\n",
    "spa = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "MAX_NUM_WORDS = 10000000\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_QUE_LENGTH = 100#EMBEDDING_DIM\n",
    "VALIDATION_SPLIT = 0.8\n",
    "NUMCONTEXT = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"char_embeddings.pickle\",\"rb\") as fd:\n",
    "    char_embeddings = pickle.load(fd)\n",
    "    \n",
    "def get_char_embedding(word):\n",
    "    x = np.zeros(EMBEDDING_DIM)\n",
    "    count = 0\n",
    "    for i in range(len(word)):\n",
    "        try:\n",
    "            count = count +1\n",
    "            temp = np.asarray(char_embeddings[word[i]])\n",
    "        except:\n",
    "            temp = np.zeros(EMBEDDING_DIM)\n",
    "        x = x+temp\n",
    "    return x/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18897\n",
      "87599\n",
      "210007\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[^\\s]+')\n",
    "\n",
    "def vectorize_stories(inp,que,ans):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for i in range(0,len(inp)):\n",
    "        inputs.append([word_index[w] for w in inp[i]])\n",
    "        queries.append([word_index[w] for w in que[i]])\n",
    "        # answers.append(ans)\n",
    "    return (pad_sequences(inputs, maxlen=MAX_SEQUENCE_LENGTH,padding='post'),\n",
    "            pad_sequences(queries, maxlen=MAX_QUE_LENGTH,padding='post'),\n",
    "            np.array(ans))\n",
    "\n",
    "def para_tokenizer(data):\n",
    "    x_tokens = tokenizer.tokenize(data)\n",
    "    spans = tokenizer.span_tokenize(data)\n",
    "    sp = [span for span in spans]\n",
    "    return x_tokens,sp\n",
    "\n",
    "def que_tokenizer(data):\n",
    "    x_tokens = tokenizer.tokenize(data)\n",
    "    return x_tokens\n",
    "\n",
    "print(len(context))\n",
    "# context = context[0:NUMCONTEXT]\n",
    "\n",
    "inp = []\n",
    "que = []\n",
    "ans = []\n",
    "i =0\n",
    "for c in context:\n",
    "    tokens,sp = para_tokenizer(c)\n",
    "    \n",
    "    q=questions[i]\n",
    "    a=answers[i]\n",
    "    all_ques = re.split('\\n', q)\n",
    "    all_ans = re.split('\\n', a)\n",
    "    all_s = re.split('\\n', spa[i])\n",
    "    for j in range (0,len(all_ques)):\n",
    "        \n",
    "        x = re.split(',',all_s[j])\n",
    "        try:\n",
    "            x = list(map(int, x))\n",
    "        except:\n",
    "            continue\n",
    "        sp1 = [span[0] for span in sp]\n",
    "        sp2 = [span[1] for span in sp]\n",
    "        st = sp1.index(min(sp1, key=lambda xx:abs(xx-x[0])))\n",
    "        en = sp2.index(min(sp2, key=lambda xx:abs(xx-x[1])))\n",
    "        inp.append(tokens)\n",
    "        que.append(que_tokenizer(all_ques[j]))\n",
    "        ans.append([st,en])\n",
    "        #ans.append(st)\n",
    "    i+=1\n",
    "\n",
    "print(len(inp))\n",
    "# print(inp[0])\n",
    "# print(que[0])\n",
    "# print(ans[1])\n",
    "\n",
    "\n",
    "vocab = set()\n",
    "for i in range(0,len(inp)):\n",
    "    vocab |= set(inp[i] + que[i])\n",
    "vocab = sorted(vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "vocab_size = len(vocab) + 1\n",
    "# story_maxlen = max(map(len, (x for x in inp)))\n",
    "# query_maxlen = max(map(len, (x for x in que)))\n",
    "# print(story_maxlen,query_maxlen)\n",
    "\n",
    "word_index = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "index_word = dict((i+1, c) for i, c in enumerate(vocab))\n",
    "train_con, train_que, answers = vectorize_stories(inp,que,ans)\n",
    "train_ans_start = to_categorical(answers[:,0],MAX_SEQUENCE_LENGTH)\n",
    "train_ans_end = to_categorical(answers[:,1],MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "split = int(NUMCONTEXT*VALIDATION_SPLIT)\n",
    "train_context = train_con[0:split]\n",
    "val_context = train_con[split:NUMCONTEXT]\n",
    "train_question = train_que[0:split]\n",
    "val_question = train_que[split:NUMCONTEXT]\n",
    "train_answer_start = train_ans_start[0:split]\n",
    "val_answer_start = train_ans_start[split:NUMCONTEXT]\n",
    "train_answer_end = train_ans_end[0:split]\n",
    "val_answer_end = train_ans_end[split:NUMCONTEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27739, 50)\n"
     ]
    }
   ],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "#     print(word,i)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = get_char_embedding(word)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.wrappers.Bidirectional object at 0x7fc7fbc179e8>\n",
      "Tensor(\"dense_29/Reshape_2:0\", shape=(?, 500, 50), dtype=float32)\n",
      "Tensor(\"dense_30/Reshape_2:0\", shape=(?, 100, 50), dtype=float32)\n",
      "Tensor(\"activation_13/truediv:0\", shape=(?, 100, 1), dtype=float32)\n",
      "Tensor(\"multiply_17/mul:0\", shape=(?, 100, 50), dtype=float32)\n",
      "Tensor(\"lambda_13/Sum:0\", shape=(?, 50), dtype=float32)\n",
      "Tensor(\"repeat_vector_9/Tile:0\", shape=(?, 500, 50), dtype=float32)\n",
      "Tensor(\"multiply_18/mul:0\", shape=(?, 500, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W = EMBEDDING_DIM\n",
    "N = MAX_SEQUENCE_LENGTH\n",
    "M = MAX_QUE_LENGTH\n",
    "dropout_rate = 0\n",
    "input_sequence = Input((MAX_SEQUENCE_LENGTH,))\n",
    "question = Input((MAX_QUE_LENGTH,))\n",
    "context_encoder = Sequential()\n",
    "context_encoder.add(Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_QUE_LENGTH,\n",
    "                            trainable=False))\n",
    "\n",
    "\n",
    "context_encoded = context_encoder(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "encoder = Bidirectional(LSTM(units=W,return_sequences=True))\n",
    "print(encoder)\n",
    "\n",
    "passage_encoding = context_encoded\n",
    "passage_encoding = encoder(passage_encoding)\n",
    "passage_encoding = Dense(W,use_bias=False,trainable=True)(passage_encoding) #(ex, MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)\n",
    "print(passage_encoding)\n",
    "\n",
    "question_encoding = question_encoded\n",
    "question_encoding = encoder(question_encoding)\n",
    "question_encoding = Dense(W,use_bias=False,trainable=True)(question_encoding) #(ex, MAX_QUE_LENGTH,EMBEDDING_DIM)\n",
    "print(question_encoding)\n",
    "## Weighted Representation of question\n",
    "\n",
    "question_attention_vector = Dense(1)(question_encoding) #(ex, MAX_QUE_LENGTH,1)\n",
    "question_attention_vector = Activation('softmax')(question_attention_vector) #(ex, MAX_QUE_LENGTH,1)\n",
    "print(question_attention_vector)\n",
    "question_attention_vector = multiply([question_encoding, question_attention_vector]) #(ex, MAX_QUE_LENGTH,MAX_QUE_LENGTH)\n",
    "print(question_attention_vector)\n",
    "\n",
    "question_attention_vector = Lambda(lambda q: K.sum(q, axis=1))(question_attention_vector) #(ex, MAX_QUE_LENGTH)\n",
    "print(question_attention_vector)\n",
    "\n",
    "##\n",
    "question_attention_vector = RepeatVector(N)(question_attention_vector) #(ex, MAX_CONTEXT_LENGTH,MAX_QUE_LENGTH)\n",
    "print(question_attention_vector)\n",
    "\n",
    "\n",
    "## FeedForward Layer to predict answer starting\n",
    "ans_st = multiply([passage_encoding, question_attention_vector])\n",
    "print(ans_st)\n",
    "answer_start = concatenate([passage_encoding,question_attention_vector, ans_st])\n",
    "\n",
    "answer_start = Dense(W, activation='relu')(answer_start)\n",
    "answer_start = Dense(1)(answer_start)\n",
    "answer_start = Flatten()(answer_start)\n",
    "answer_start = Activation('softmax')(answer_start)\n",
    "\n",
    "\n",
    "##Passing starting embedding of answer predicted\n",
    "x = Lambda(lambda x: K.argmax(x,axis=1))(answer_start)\n",
    "start_feature = Lambda(lambda arg: K.tf.gather_nd(arg[0], K.tf.stack(\n",
    "    [K.tf.range(K.tf.shape(arg[1])[0]), K.tf.cast(arg[1], K.tf.int32)], axis=1)))([passage_encoding, x])\n",
    "\n",
    "start_feature = RepeatVector(N)(start_feature)\n",
    "##\n",
    "\n",
    "\n",
    "## FeedForward Layer to predict answer ending\n",
    "ans_1 = multiply([passage_encoding, question_attention_vector])\n",
    "ans_2 = multiply([passage_encoding, start_feature])\n",
    "answer_end = concatenate([passage_encoding,question_attention_vector,start_feature, ans_1,ans_2])\n",
    "\n",
    "answer_end = Dense(W, activation='relu')(answer_end)\n",
    "answer_end = Dense(1)(answer_end)\n",
    "answer_end = Flatten()(answer_end)\n",
    "answer_end = Activation('softmax')(answer_end)\n",
    "##\n",
    "\n",
    "\n",
    "inputs = [input_sequence, question]\n",
    "outputs = [answer_start, answer_end]\n",
    "model = Model(inputs,outputs)\n",
    "# model.summary()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 500) (800, 100) (800, 500) (800, 500)\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 32s - loss: 10.1938 - activation_2_loss: 5.0734 - activation_3_loss: 5.1204 - activation_2_acc: 0.0388 - activation_3_acc: 0.0325 - val_loss: 8.6354 - val_activation_2_loss: 4.3092 - val_activation_3_loss: 4.3262 - val_activation_2_acc: 0.1050 - val_activation_3_acc: 0.0550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc7f856c6a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_context.shape,train_question.shape,train_answer_start.shape,train_answer_end.shape)\n",
    "model.fit([train_context, train_question], [train_answer_start,train_answer_end],\n",
    "          batch_size=30,\n",
    "          epochs=1,\n",
    "          validation_data=([val_context, val_question], [val_answer_start,val_answer_end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
