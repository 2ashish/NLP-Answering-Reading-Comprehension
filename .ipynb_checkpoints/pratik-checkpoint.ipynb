{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/pratik/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, RepeatVector, Masking, Dropout, Flatten, Activation, Reshape, Lambda, Permute, merge, multiply, concatenate\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_file = open(os.path.join('./data/', 'train_context'), 'r')\n",
    "c = context_file.read()\n",
    "context = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "question_file = open(os.path.join('./data/', 'train_question'), 'r')\n",
    "c = question_file.read()\n",
    "questions = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "answer_file = open(os.path.join('./data/', 'train_answer'), 'r')\n",
    "c = answer_file.read()\n",
    "answers = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "span_file = open(os.path.join('./data/', 'train_span'), 'r')\n",
    "c = span_file.read()\n",
    "spa = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "MAX_NUM_WORDS = 100000\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_QUE_LENGTH = EMBEDDING_DIM\n",
    "VALIDATION_SPLIT = 0.8\n",
    "NUMCONTEXT = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"char_embeddings.pickle\",\"rb\") as fd:\n",
    "    char_embeddings = pickle.load(fd)\n",
    "    \n",
    "def get_char_embedding(word):\n",
    "    x = np.zeros(EMBEDDING_DIM)\n",
    "    count = 0\n",
    "    for i in range(len(word)):\n",
    "        try:\n",
    "            count = count +1\n",
    "            temp = np.asarray(char_embeddings[word[i]])\n",
    "        except:\n",
    "            temp = np.zeros(EMBEDDING_DIM)\n",
    "        x = x+temp\n",
    "    return x/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# print(embeddings_index['bhangale'])\n",
    "# print(type(get_char_embedding('bhangale')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5825\n",
      "27738\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[^\\s]+')\n",
    "\n",
    "def vectorize_stories(inp,que,ans):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for i in range(0,len(inp)):\n",
    "        inputs.append([word_index[w] for w in inp[i]])\n",
    "        queries.append([word_index[w] for w in que[i]])\n",
    "        # answers.append(ans)\n",
    "    return (pad_sequences(inputs, maxlen=MAX_SEQUENCE_LENGTH,padding='post'),\n",
    "            pad_sequences(queries, maxlen=MAX_QUE_LENGTH,padding='post'),\n",
    "            np.array(ans))\n",
    "\n",
    "def para_tokenizer(data):\n",
    "    x_tokens = tokenizer.tokenize(data)\n",
    "    spans = tokenizer.span_tokenize(data)\n",
    "    sp = [span for span in spans]\n",
    "    return x_tokens,sp\n",
    "\n",
    "def que_tokenizer(data):\n",
    "    x_tokens = tokenizer.tokenize(data)\n",
    "    return x_tokens\n",
    "\n",
    "\n",
    "context = context[0:NUMCONTEXT]\n",
    "\n",
    "inp = []\n",
    "que = []\n",
    "ans = []\n",
    "i =0\n",
    "for c in context:\n",
    "    tokens,sp = para_tokenizer(c)\n",
    "    \n",
    "    q=questions[i]\n",
    "    a=answers[i]\n",
    "    all_ques = re.split('\\n', q)\n",
    "    all_ans = re.split('\\n', a)\n",
    "    all_s = re.split('\\n', spa[i])\n",
    "    for j in range (0,len(all_ques)):\n",
    "        inp.append(tokens)\n",
    "        x = re.split(',',all_s[j])\n",
    "        x = list(map(int, x))\n",
    "        k = 0\n",
    "        for span in sp:\n",
    "            if span[0] <= x[0] <= span[1]:\n",
    "                st = k\n",
    "            if span[0] <= x[1] <= span[1]:\n",
    "                en = k\n",
    "            k+=1\n",
    "        que.append(que_tokenizer(all_ques[j]))\n",
    "        ans.append([st,en])\n",
    "        #ans.append(st)\n",
    "    i+=1\n",
    "\n",
    "print(len(inp))\n",
    "# print(inp[0])\n",
    "# print(que[0])\n",
    "# print(ans[1])\n",
    "\n",
    "\n",
    "vocab = set()\n",
    "for i in range(0,len(inp)):\n",
    "    vocab |= set(inp[i] + que[i])\n",
    "vocab = sorted(vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "vocab_size = len(vocab) + 1\n",
    "# story_maxlen = max(map(len, (x for x in inp)))\n",
    "# query_maxlen = max(map(len, (x for x in que)))\n",
    "# print(story_maxlen,query_maxlen)\n",
    "\n",
    "word_index = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "index_word = dict((i+1, c) for i, c in enumerate(vocab))\n",
    "train_con, train_que, answers = vectorize_stories(inp,que,ans)\n",
    "train_ans_start = to_categorical(answers[:,0],MAX_SEQUENCE_LENGTH)\n",
    "train_ans_end = to_categorical(answers[:,1],MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "split = int(NUMCONTEXT*VALIDATION_SPLIT)\n",
    "train_context = train_con[0:split]\n",
    "val_context = train_con[split+1:NUMCONTEXT]\n",
    "train_question = train_que[0:split]\n",
    "val_question = train_que[split+1:NUMCONTEXT]\n",
    "train_answer_start = train_ans_start[0:split]\n",
    "val_answer_start = train_ans_start[split+1:NUMCONTEXT]\n",
    "train_answer_end = train_ans_end[0:split]\n",
    "val_answer_end = train_ans_end[split+1:NUMCONTEXT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-9dfbf335e141>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'context_char.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr_embedded_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with open('context_char.pickle','wb') as fd:\n",
    "#     pickle.dump(chr_embedded_context,fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27739, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "#     print(word,i)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = get_char_embedding(word)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda_1/truediv:0\", shape=(?, 50, 1), dtype=float32)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 50)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)        (None, 500, 50)       1386950     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)        (None, 50, 50)        1386950     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  multiple              40400       sequential_1[1][0]               \n",
      "                                                                   sequential_2[1][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50, 50)        5000        bidirectional_1[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 50, 1)         51          dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 50, 1)         0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 50, 50)        0           dense_2[0][0]                    \n",
      "                                                                   lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 50)            0           lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 500, 50)       5000        bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 500, 50)       0           lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)            (None, 500, 50)       0           dense_1[0][0]                    \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 500, 150)      0           dense_1[0][0]                    \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "                                                                   multiply_1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 500, 50)       7550        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 500, 1)        51          dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 500)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 500)           0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None,)               0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (None, 50)            0           dense_1[0][0]                    \n",
      "                                                                   lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)   (None, 500, 50)       0           lambda_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)            (None, 500, 50)       0           dense_1[0][0]                    \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)            (None, 500, 50)       0           dense_1[0][0]                    \n",
      "                                                                   repeat_vector_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 500, 250)      0           dense_1[0][0]                    \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "                                                                   repeat_vector_2[0][0]            \n",
      "                                                                   multiply_2[0][0]                 \n",
      "                                                                   multiply_3[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 500, 50)       12550       concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 500, 1)        51          dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 500)           0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 500)           0           flatten_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,844,553\n",
      "Trainable params: 70,653\n",
      "Non-trainable params: 2,773,900\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "W = EMBEDDING_DIM\n",
    "N = MAX_SEQUENCE_LENGTH\n",
    "M = MAX_QUE_LENGTH\n",
    "dropout_rate = 0\n",
    "input_sequence = Input((MAX_SEQUENCE_LENGTH,))\n",
    "question = Input((MAX_QUE_LENGTH,))\n",
    "context_encoder = Sequential()\n",
    "context_encoder.add(Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_QUE_LENGTH,\n",
    "                            trainable=False))\n",
    "\n",
    "\n",
    "context_encoded = context_encoder(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "encoder = Bidirectional(LSTM(units=W,return_sequences=True))\n",
    "\n",
    "passage_encoding = context_encoded\n",
    "passage_encoding = encoder(passage_encoding)\n",
    "passage_encoding = Dense(W,use_bias=False,trainable=True)(passage_encoding)\n",
    "\n",
    "question_encoding = question_encoded\n",
    "question_encoding = encoder(question_encoding)\n",
    "question_encoding = Dense(W,use_bias=False,trainable=True)(question_encoding)\n",
    "\n",
    "question_attention_vector = Dense(1)(question_encoding)\n",
    "# question_attention_vector = Activation('softmax')(question_attention_vector)\n",
    "question_attention_vector = Lambda(lambda q: keras.activations.softmax(q, axis=1))(question_attention_vector)\n",
    "print(question_attention_vector)\n",
    "\n",
    "question_attention_vector = Lambda(lambda q: q[0] * q[1])([question_encoding, question_attention_vector])\n",
    "question_attention_vector = Lambda(lambda q: K.sum(q, axis=1))(question_attention_vector)\n",
    "question_attention_vector = RepeatVector(N)(question_attention_vector)\n",
    "\n",
    "ans_st = multiply([passage_encoding, question_attention_vector])\n",
    "answer_start = concatenate([passage_encoding,question_attention_vector, ans_st])\n",
    "\n",
    "answer_start = Dense(W, activation='relu')(answer_start)\n",
    "answer_start = Dense(1)(answer_start)\n",
    "answer_start = Flatten()(answer_start)\n",
    "answer_start = Activation('softmax')(answer_start)\n",
    "def s_answer_feature(x):\n",
    "    maxind = K.argmax(\n",
    "        x,\n",
    "        axis=1,\n",
    "    )\n",
    "    return maxind\n",
    "\n",
    "x = Lambda(lambda x: K.tf.cast(s_answer_feature(x), dtype=K.tf.int32))(answer_start)\n",
    "start_feature = Lambda(lambda arg: K.tf.gather_nd(arg[0], K.tf.stack(\n",
    "    [K.tf.range(K.tf.shape(arg[1])[0]), K.tf.cast(arg[1], K.tf.int32)], axis=1)))([passage_encoding, x])\n",
    "start_feature = RepeatVector(N)(start_feature)\n",
    "\n",
    "\n",
    "ans_1 = multiply([passage_encoding, question_attention_vector])\n",
    "ans_2 = multiply([passage_encoding, start_feature])\n",
    "answer_end = concatenate([passage_encoding,question_attention_vector,start_feature, ans_1,ans_2])\n",
    "\n",
    "answer_end = Dense(W, activation='relu')(answer_end)\n",
    "answer_end = Dense(1)(answer_end)\n",
    "answer_end = Flatten()(answer_end)\n",
    "answer_end = Activation('softmax')(answer_end)\n",
    "\n",
    "inputs = [input_sequence, question]\n",
    "outputs = [answer_start, answer_end]\n",
    "model = Model(inputs,outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 500) (800, 50) (800, 500) (800, 500)\n",
      "Train on 800 samples, validate on 199 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 30s - loss: 9.8525 - activation_1_loss: 5.0161 - activation_2_loss: 4.8363 - activation_1_acc: 0.0288 - activation_2_acc: 0.0388 - val_loss: 8.6281 - val_activation_1_loss: 4.4119 - val_activation_2_loss: 4.2162 - val_activation_1_acc: 0.0553 - val_activation_2_acc: 0.0503\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 28s - loss: 8.8074 - activation_1_loss: 4.5005 - activation_2_loss: 4.3069 - activation_1_acc: 0.0488 - activation_2_acc: 0.0675 - val_loss: 8.3438 - val_activation_1_loss: 4.2839 - val_activation_2_loss: 4.0599 - val_activation_1_acc: 0.0804 - val_activation_2_acc: 0.0854\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 28s - loss: 8.4642 - activation_1_loss: 4.3279 - activation_2_loss: 4.1363 - activation_1_acc: 0.0700 - activation_2_acc: 0.0700 - val_loss: 8.1323 - val_activation_1_loss: 4.1718 - val_activation_2_loss: 3.9606 - val_activation_1_acc: 0.0955 - val_activation_2_acc: 0.0854\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 28s - loss: 8.2834 - activation_1_loss: 4.2264 - activation_2_loss: 4.0570 - activation_1_acc: 0.0875 - activation_2_acc: 0.0888 - val_loss: 8.1539 - val_activation_1_loss: 4.1966 - val_activation_2_loss: 3.9573 - val_activation_1_acc: 0.0955 - val_activation_2_acc: 0.0804\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 28s - loss: 8.1393 - activation_1_loss: 4.1563 - activation_2_loss: 3.9831 - activation_1_acc: 0.0875 - activation_2_acc: 0.0938 - val_loss: 8.1507 - val_activation_1_loss: 4.1748 - val_activation_2_loss: 3.9759 - val_activation_1_acc: 0.0955 - val_activation_2_acc: 0.1055\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 28s - loss: 8.0039 - activation_1_loss: 4.0837 - activation_2_loss: 3.9202 - activation_1_acc: 0.0975 - activation_2_acc: 0.1025 - val_loss: 8.2277 - val_activation_1_loss: 4.2336 - val_activation_2_loss: 3.9941 - val_activation_1_acc: 0.0854 - val_activation_2_acc: 0.0804\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 28s - loss: 7.8820 - activation_1_loss: 4.0175 - activation_2_loss: 3.8645 - activation_1_acc: 0.1088 - activation_2_acc: 0.1113 - val_loss: 8.0303 - val_activation_1_loss: 4.1154 - val_activation_2_loss: 3.9149 - val_activation_1_acc: 0.0955 - val_activation_2_acc: 0.0905\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 28s - loss: 7.7395 - activation_1_loss: 3.9416 - activation_2_loss: 3.7979 - activation_1_acc: 0.1000 - activation_2_acc: 0.0988 - val_loss: 8.1856 - val_activation_1_loss: 4.1896 - val_activation_2_loss: 3.9960 - val_activation_1_acc: 0.1005 - val_activation_2_acc: 0.0804\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 31s - loss: 7.5817 - activation_1_loss: 3.8579 - activation_2_loss: 3.7238 - activation_1_acc: 0.1088 - activation_2_acc: 0.1213 - val_loss: 8.1729 - val_activation_1_loss: 4.1948 - val_activation_2_loss: 3.9781 - val_activation_1_acc: 0.0955 - val_activation_2_acc: 0.0704\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 31s - loss: 7.3672 - activation_1_loss: 3.7583 - activation_2_loss: 3.6089 - activation_1_acc: 0.1388 - activation_2_acc: 0.1425 - val_loss: 8.2883 - val_activation_1_loss: 4.1942 - val_activation_2_loss: 4.0942 - val_activation_1_acc: 0.1106 - val_activation_2_acc: 0.0905\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 29s - loss: 7.1967 - activation_1_loss: 3.6576 - activation_2_loss: 3.5390 - activation_1_acc: 0.1525 - activation_2_acc: 0.1563 - val_loss: 8.0688 - val_activation_1_loss: 4.1029 - val_activation_2_loss: 3.9659 - val_activation_1_acc: 0.0905 - val_activation_2_acc: 0.0955\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 29s - loss: 6.9642 - activation_1_loss: 3.5356 - activation_2_loss: 3.4286 - activation_1_acc: 0.1713 - activation_2_acc: 0.1700 - val_loss: 8.1130 - val_activation_1_loss: 4.1180 - val_activation_2_loss: 3.9950 - val_activation_1_acc: 0.1005 - val_activation_2_acc: 0.0905\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 28s - loss: 6.8136 - activation_1_loss: 3.4572 - activation_2_loss: 3.3563 - activation_1_acc: 0.1813 - activation_2_acc: 0.1913 - val_loss: 8.2552 - val_activation_1_loss: 4.2110 - val_activation_2_loss: 4.0442 - val_activation_1_acc: 0.1156 - val_activation_2_acc: 0.0955\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 28s - loss: 6.6562 - activation_1_loss: 3.3767 - activation_2_loss: 3.2795 - activation_1_acc: 0.2038 - activation_2_acc: 0.1925 - val_loss: 8.1381 - val_activation_1_loss: 4.1544 - val_activation_2_loss: 3.9837 - val_activation_1_acc: 0.1106 - val_activation_2_acc: 0.1005\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 29s - loss: 6.4819 - activation_1_loss: 3.2821 - activation_2_loss: 3.1998 - activation_1_acc: 0.2138 - activation_2_acc: 0.1988 - val_loss: 8.1959 - val_activation_1_loss: 4.1690 - val_activation_2_loss: 4.0269 - val_activation_1_acc: 0.1156 - val_activation_2_acc: 0.0955\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 28s - loss: 6.3138 - activation_1_loss: 3.2009 - activation_2_loss: 3.1129 - activation_1_acc: 0.2213 - activation_2_acc: 0.2188 - val_loss: 8.4052 - val_activation_1_loss: 4.2921 - val_activation_2_loss: 4.1131 - val_activation_1_acc: 0.1156 - val_activation_2_acc: 0.0905\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 28s - loss: 6.1226 - activation_1_loss: 3.1061 - activation_2_loss: 3.0165 - activation_1_acc: 0.2475 - activation_2_acc: 0.2450 - val_loss: 8.3447 - val_activation_1_loss: 4.2455 - val_activation_2_loss: 4.0992 - val_activation_1_acc: 0.1206 - val_activation_2_acc: 0.0905\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 29s - loss: 5.9695 - activation_1_loss: 3.0308 - activation_2_loss: 2.9388 - activation_1_acc: 0.2275 - activation_2_acc: 0.2413 - val_loss: 8.6590 - val_activation_1_loss: 4.3690 - val_activation_2_loss: 4.2901 - val_activation_1_acc: 0.1156 - val_activation_2_acc: 0.0955\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 28s - loss: 5.8015 - activation_1_loss: 2.9438 - activation_2_loss: 2.8577 - activation_1_acc: 0.2575 - activation_2_acc: 0.2538 - val_loss: 8.7914 - val_activation_1_loss: 4.4423 - val_activation_2_loss: 4.3491 - val_activation_1_acc: 0.1156 - val_activation_2_acc: 0.1106\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 29s - loss: 5.5817 - activation_1_loss: 2.8303 - activation_2_loss: 2.7513 - activation_1_acc: 0.2738 - activation_2_acc: 0.2850 - val_loss: 9.2532 - val_activation_1_loss: 4.5913 - val_activation_2_loss: 4.6619 - val_activation_1_acc: 0.1256 - val_activation_2_acc: 0.0905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc054605748>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_context.shape,train_question.shape,train_answer_start.shape,train_answer_end.shape)\n",
    "model.fit([train_context, train_question], [train_answer_start,train_answer_end],\n",
    "          batch_size=30,\n",
    "          epochs=20,\n",
    "          validation_data=([val_context, val_question], [val_answer_start,val_answer_end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
