{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squad_dataset = json.load(open('SQuAD/train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sequence):\n",
    "    # tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"') for token in nltk.word_tokenize(sequence)]\n",
    "    tokens = nltk.word_tokenize(sequence)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def token_map(context, context_tokens):\n",
    "    word = ''\n",
    "    current_token_id = 0\n",
    "    token_map = dict()\n",
    "\n",
    "    for cid, c in enumerate(context):\n",
    "        if c != ' ':\n",
    "            word += c\n",
    "            context_token = context_tokens[current_token_id]\n",
    "            if word == context_token:\n",
    "                start = cid - len(word) + 1\n",
    "                token_map[start] = [word, current_token_id]\n",
    "                word = ''\n",
    "                current_token_id += 1\n",
    "    return token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset):\n",
    "    with open(os.path.join('./data/', 'train_context'), 'w') as context_file, \\\n",
    "    open(os.path.join('./data/', 'train_question'), 'w') as question_file, \\\n",
    "    open(os.path.join('./data/', 'train_answer'), 'w') as answer_file, \\\n",
    "    open(os.path.join('./data/', 'train_span'), 'w') as span_file:\n",
    "        for data_id in range(len(dataset['data'])):\n",
    "            data = dataset['data'][data_id]\n",
    "            for para_id in range(len(data['paragraphs'])):\n",
    "                para = data['paragraphs'][para_id]\n",
    "                context = para['context']\n",
    "                if context[0] == '\\n':\n",
    "                    context = context[-len(context)+1:]\n",
    "                qas = para['qas']\n",
    "                for qid in range(len(qas)):\n",
    "                    question = qas[qid]['question']\n",
    "                    answer_text = qas[qid]['answers'][0]['text']\n",
    "                    answer_start = qas[qid]['answers'][0]['answer_start']\n",
    "                    answer_end = answer_start + len(answer_text)\n",
    "                    \n",
    "                    question_file.write(question + '\\n')\n",
    "                    answer_file.write(answer_text + '\\n')\n",
    "                    span_file.write(str(answer_start) + ',' + str(answer_end) + '\\n')\n",
    "                \n",
    "                context_file.write(context + '\\n' + '-'*30 + '\\n')\n",
    "                question_file.write('-'*30 + '\\n')\n",
    "                answer_file.write('-'*30 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_input_file = './glove/glove.6B.300d.txt'\n",
    "word2vec_output_file = './glove/glove.6B.300d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_output_file = './glove/glove.6B.300d.txt.word2vec'\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_rep(x_train, q_data, model):\n",
    "    x_avg = []\n",
    "    q_avg = []\n",
    "    avg = []\n",
    "    for x in x_train:\n",
    "        x_tokens = nltk.word_tokenize(x)\n",
    "        x_vec = []\n",
    "        for x_token in x_tokens:\n",
    "            if x_token in model.wv.vocab:\n",
    "                x_vec.append(model.wv[x_token])\n",
    "        x_avg.append([a/len(x_vec) for a in sum(x_vec)])\n",
    "    for ques in q_data:\n",
    "        q_vec = []\n",
    "        for q in ques:\n",
    "            q_tokens = nltk.word_tokenize(q)\n",
    "            for q_token in q_tokens:\n",
    "                if q_token in model.wv.vocab:\n",
    "                    q_vec.append(model.wv[q_token])\n",
    "        q_avg.append([a/len(q_vec) for a in sum(q_vec)])\n",
    "    for i in range(len(x_avg)):\n",
    "        avg.append([(a+b)/2 for (a,b) in zip(x_avg[i], q_avg[i])])\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_data(squad_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(text):\n",
    "    if text[0] == '\\n':\n",
    "        text = text[-len(text)+1:]\n",
    "    elif text[-1] == '\\n':\n",
    "        text = text[:len(text)-1]\n",
    "    return text\n",
    "\n",
    "def get_cdata(context_file):\n",
    "    c = context_file.read()\n",
    "    lines = re.split('\\n' + '-'*30 + '\\n', c)[:-1]\n",
    "    return lines\n",
    "\n",
    "def get_qdata(xfile):\n",
    "    final = []\n",
    "    line = []\n",
    "    for l in xfile:\n",
    "        l = remove_newlines(l)\n",
    "        if l != '-'*30:\n",
    "            line.append(l)\n",
    "        else:\n",
    "            final.append(line)\n",
    "            line = []\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_file = open('./data/train_context', 'r')\n",
    "questions_file = open('./data/train_question', 'r')\n",
    "context_data = get_cdata(context_file)\n",
    "questions_data = get_qdata(questions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohithm/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/rohithm/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/rohithm/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/rohithm/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "glove_rep = average_rep(context_data[:2500], question_data[:2500], glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18896\n",
      "19028\n",
      "19028\n",
      "18896\n"
     ]
    }
   ],
   "source": [
    "# Check lengths\n",
    "tot = 0\n",
    "for data_id in range(len(squad_dataset['data'])):\n",
    "    data = squad_dataset['data'][data_id]\n",
    "    for para_id in range(len(data['paragraphs'])):\n",
    "        para = data['paragraphs'][para_id]\n",
    "        tot += 1\n",
    "\n",
    "print(tot)\n",
    "print(c_tot)\n",
    "print(len(context_data))\n",
    "print(len(questions_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
