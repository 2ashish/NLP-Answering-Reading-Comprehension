{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_input_file = './glove/glove.6B.50d.txt'\n",
    "# word2vec_output_file = './glove/glove.6B.50d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "# word2vec_output_file = './glove/glove.6B.50d.txt.word2vec'\n",
    "# glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_output_file = './glove/glove.6B.50d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = 50\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "def our_tokenizer(data):\n",
    "    x_tokens = tokenizer.tokenize(data.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(w) for w in x_tokens if not w in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"As at most other universities, Notre Dame's students run a number of news media outlets.\", 'The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals.', 'Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States.', 'The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork.', 'The Dome yearbook is published annually.', \"The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College.\", 'Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University.', 'In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published.', 'Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production.', 'Neither paper is published as often as The Observer; however, all three are distributed to all students.', 'Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.']\n",
      "['When did the Scholastic Magazine of Notre dame begin publishing?', \"How often is Notre Dame's the Juggler published?\", 'What is the daily student paper at Notre Dame called?', 'How many student news papers are found at Notre Dame?', 'In what year did the student paper Common Sense begin publication at Notre Dame?']\n",
      "[0.7979941  0.71293193 0.79847485 0.77435875 0.7092277  0.8236397\n",
      " 0.7648379  0.70788646 0.6634198  0.68856525 0.7964657 ] 5\n",
      "[0.7872409  0.643538   0.6830186  0.7491208  0.56479913 0.7870083\n",
      " 0.6253291  0.7169851  0.6486937  0.68856263 0.73827237] 0\n",
      "[0.9270047  0.8303125  0.8333212  0.81619537 0.6920966  0.91983426\n",
      " 0.7786869  0.83732176 0.81036043 0.8222982  0.83933765] 0\n",
      "[0.9295075  0.85849595 0.85234064 0.84531766 0.68409884 0.9231593\n",
      " 0.7742029  0.862906   0.8633395  0.8858309  0.86850035] 0\n",
      "[0.90659803 0.81544286 0.8935653  0.8714175  0.6977719  0.90440637\n",
      " 0.78569    0.88209504 0.87541056 0.8838741  0.92855287] 10\n"
     ]
    }
   ],
   "source": [
    "context_file = open(os.path.join('./data/', 'train_context'), 'r')\n",
    "c = context_file.read()\n",
    "context = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "question_file = open(os.path.join('./data/', 'train_question'), 'r')\n",
    "c = question_file.read()\n",
    "questions = re.split('\\n' + '-' + '\\n', c)\n",
    "del c\n",
    "\n",
    "Average_vector = MeanEmbeddingVectorizer(glove_model)\n",
    "\n",
    "c = context[1]\n",
    "q = questions[1]\n",
    "\n",
    "all_sent = sent_tokenize(c)\n",
    "all_ques = re.split('\\n', q)\n",
    "\n",
    "sent_tokens = []\n",
    "for sent in all_sent:\n",
    "    tokens = our_tokenizer(sent)\n",
    "    sent_tokens.append(tokens)\n",
    "\n",
    "sent_vec = Average_vector.transform(sent_tokens)\n",
    "\n",
    "\n",
    "que_tokens = []\n",
    "for que in all_ques:\n",
    "    tokens = our_tokenizer(que)\n",
    "    que_tokens.append(tokens)\n",
    "\n",
    "ques_vec = Average_vector.transform(que_tokens)\n",
    "\n",
    "x = cosine_similarity(ques_vec,sent_vec)\n",
    "print(all_sent)\n",
    "print(all_ques)\n",
    "for i in range(len(x)):\n",
    "    index = np.argmax(x[i])\n",
    "    print(x[i],index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
